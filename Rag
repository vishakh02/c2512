PURPOSE

Handles text storage, vector embedding, retrieval, and response generation for an efficient RAG pipeline.

FEATURES

Stores user-entered text as vector embeddings for retrieval.

Processes queries using vector search and a large language model (LLM).

Generates responses by retrieving relevant stored data and passing it to Ollama.


DEPENDENCIES

Embedding Model (all-MiniLM-L6-V2) → Converts text into vector embeddings for efficient search.

FAISS / ChromaDB → Stores vector embeddings and performs similarity-based retrieval.

LangChain → Manages interaction between RAG components (vector DB, embedding model, LLM).

Ollama LLM → Generates responses based on retrieved vectorized data.

NumPy / SciPy → Supports mathematical operations for vector search and retrieval.


DATA FLOW

1. Text Ingestion & Storage

Receives user-entered text from FastAPI.

Embeds text into vectors using all-MiniLM-L6-V2.

Stores the vectors in FAISS/ChromaDB.

Confirms storage to FastAPI once embedding & storage are complete.



2. Query Processing & Response Generation

Receives query from FastAPI.

Converts query into a vector using the same embedding model.

Retrieves similar stored vectors from FAISS/ChromaDB.

Passes retrieved data to Ollama for response generation.

Sends generated response back to FastAPI.




WAITING REQUIREMENTS

Waits for vector embedding process to complete before confirming storage.

Waits for Ollama to generate a response before sending it to FastAPI.
